{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv-vQeDXZh1c"
      },
      "source": [
        "# **Documentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4JD94mo9OtU"
      },
      "source": [
        "##**Answer the questions**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLPIuKL4INRl"
      },
      "source": [
        "**1-Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?**\n",
        "\n",
        "`-No :  `\n",
        "`because RNNs are better suited to analyzing temporal, sequential data, such as text or videos. A CNN has a different architecture from an RNN. CNNs are \"feed-forward neural networks\" that use filters and pooling layers, whereas RNNs feed results back into the network\n",
        "When the input image is of size (28x28x3) pixels, a fully connected neural network will have 2352 weights in the first hidden layer. In real life, the images have at least 200x200x3 pixels which results in 120,000 weights in the first hidden layer itself. Having so many parameters will result in overfitting. In such a scenario`\n",
        "\n",
        "`-No :Fully connected neural networks aren't good for feature extraction.so its not good for image \n",
        "and fully-connected model is not appropriate when our model or one of our layers has more than one input or output, we need to do layer sharing or we want to use a non-linear topology e.g. residual or skip-connection\n",
        "`\n",
        "\n",
        "`so CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier`\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDyyLJan3qmi"
      },
      "source": [
        "**2-What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?**\n",
        "\n",
        "`-In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient. Alternatively, if the derivatives are small then the gradient will decrease exponentially as we propagate through the model until it eventually vanishes, and this is the vanishing gradient problem.`\n",
        "\n",
        "`-Vanishing Gradient occurs when the derivative or slope will get smaller and smaller as we go backward with every layer during backpropagation.`\n",
        "\n",
        "`-Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients.`\n",
        "\n",
        "\n",
        "`-LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process`\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "LSTM (short for long short-term memory) primarily solves \n",
        "the vanishing gradient problem in backpropagation. \n",
        "LSTMs use a gating mechanism that controls the memoizing process. \n",
        "LSTM ’s and GRU’s were created as the solution to short-term memory. \n",
        "They have internal mechanisms called gates that can regulate the flow of information.\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEabL0y8I4mU"
      },
      "source": [
        "**3-What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?**\n",
        "\n",
        "\n",
        "`-Multi-task Learning (MTL) is a collection of techniques intended to learn multiple tasks simultaneously instead of learning them separately.`\n",
        "\n",
        "`-Multitasking refers to either the ability to pay attention to several pieces of information at the same time or the process of performing more than one task at the same time`\n",
        "\n",
        "```\n",
        "Multi-task learning (MTL) is a subfield of machine learning \n",
        "in which multiple learning tasks are solved at the same time, \n",
        "while exploiting commonalities and differences across tasks\n",
        "\n",
        "```\n",
        "\n",
        "`-multi-objective to achieve multiple targets in this assignment like predict price and type`\n",
        "\n",
        "`-Multimodal machine learning aims to build models that can process and relate information from multiple modalities.`\n",
        "\n",
        "`- in this assignment we use one model to work on text and image at the same time Inputs (two modalities): (text data) (image data)`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv6e5RpZNuWL"
      },
      "source": [
        "**4-What is the difference among xgboost, lightgbm and catboost**\n",
        "\n",
        "`-XGBoost, CatBoost, and LightGBM have emerged as the most optimized boosting techniques for gradient-boosted tree algorithms`\n",
        "\n",
        "```\n",
        "-XGBoost:\n",
        "XGBoost cannot handle categorical\n",
        "features by itself, it only accepts numerical values similar\n",
        "to Random Forest. Therefore one has to perform various encodings\n",
        "like label encoding, mean encoding or one-hot encoding before\n",
        "supplying categorical data to XGBoost\n",
        "```\n",
        "\n",
        "```\n",
        "CatBoost:\n",
        "CatBoost has the flexibility of giving indices of categorical \n",
        "columns so that it can be encoded as one-hot encoding \n",
        "using one_hot_max_size (Use one-hot encoding for all \n",
        "features with number of different values less than or equal \n",
        "to the given parameter value)\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "LightGBM:\n",
        "Similar to CatBoost, LightGBM can also handle categorical \n",
        "features by taking the input of feature names. \n",
        "It does not convert to one-hot coding, and is much faster than \n",
        "one-hot coding. LGBM uses a special algorithm to find the split value of categorical features\n",
        "```\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Formulation:**"
      ],
      "metadata": {
        "id": "BkGufTwn63Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Define the problem?**\n",
        "\n",
        "```\n",
        "the problem is when people prepare to post \n",
        "a new listing on airbnb is, how much should one ask for? \n",
        "so we are going to predict the listing price based on the \n",
        "listing characteristics and some images (In this problem, \n",
        "the task is to determine an appropriate listing price for \n",
        "a new listing on Airbnb. Instead of predicting the actual \n",
        "listing price using regression, the objective is to predict \n",
        "which pricing bin or range is appropriate for a new listing. \n",
        "A multi-objective approach is used to predict the price range \n",
        "of a new listing, along with the type of rental being advertised. \n",
        "To do so, a multi-modality approach is also applied, whereby \n",
        "images of a listing along with an accompanying text \n",
        "summary are used for training.)\n",
        "```\n",
        "* **What is the input?**\n",
        "\n",
        "```\n",
        "the input is images for different type of buildings (house,apartment and so on )\n",
        "with its description \n",
        "```\n",
        "* **What is the output?**\n",
        "\n",
        "```\n",
        "the output is predict type and price based on the input image and sammry \n",
        "```\n",
        "* **What data mining function is required?**\n",
        "\n",
        "```\n",
        "In this problem we use to make classification so i used softmax \n",
        "as activation function to make classification\n",
        "```\n",
        "* **What could be the challenges?**\n",
        "\n",
        "```\n",
        "the challenges is data has alot of noise and the images not same size \n",
        "so we should do some preprocessing before using this data \n",
        "text data has mulible languages so it need to be translated \n",
        "```\n",
        "* **What is the impact?**\n",
        "\n",
        "```\n",
        "the impact is optimize user experience and lower the bar to be a new host\n",
        "\n",
        "```\n",
        "* **What is an ideal solution?**\n",
        "\n",
        "```\n",
        "the ideal solution for this problem that model predict price and type \n",
        "for any new post perfectly and gets optimal accuracy\n",
        "```\n",
        "* **What is the experimental protocol used and how was it carried out?**\n",
        "\n",
        "```\n",
        "the training and test datasets for a set of rental \n",
        "properties are loaded into pandas dataframes. Images are \n",
        "loaded as greyscale (2 channels), and re-sized into size 64x64x2. \n",
        "Text summaries are also loaded as strings.\n",
        "```\n",
        "\n",
        "```\n",
        "Then, the text data is preprocessed by first fitting a \n",
        "tokenizer on all of the text summaries. This creates a \n",
        "vocabulary of 40000 unique words. This tokenizer is used \n",
        "to generate a sequence of integers for each summary using \n",
        "this vocabulary. A max sequence length of 100 words is enforced \n",
        "to truncate long sequences, and to pad shorter sequences with zeros \n",
        "to maintain consistent input dimensions.\n",
        "```\n",
        " `Embeddings are generated for the input text, and the average of the embeddings is computed for each sequence. A feature vector is created for the input images by passing their values through a 2D convolutional layer, using 2D max-pooling to reduce the size, and then flattening that output. These output vectors are concatenated and passed to a dense layer using softmax activation to perform separate multi-class classification tasks on price and type labels`\n",
        "\n",
        " --------"
      ],
      "metadata": {
        "id": "rT64tpB36vIQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJrJBzG70N_m"
      },
      "source": [
        "### **Install some libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bABC8V6zz4hB"
      },
      "source": [
        "Install translate library becouse the data has multi language ,so I will translate all to english "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53LjufUfSwVm"
      },
      "outputs": [],
      "source": [
        "# !pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwQsYctjQWNy"
      },
      "outputs": [],
      "source": [
        "# !pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhicj9dmAH5M",
        "outputId": "a488d1a3-18a4-4034-c5ae-10dd1293059a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1xNgHB0i9i"
      },
      "source": [
        "### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzshikLlYCq4"
      },
      "outputs": [],
      "source": [
        "import re # regax library\n",
        "from googletrans import Translator #google translate library\n",
        "# Ignore  the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "%matplotlib inline  \n",
        "# style.use('fivethirtyeight')\n",
        "# sns.set(color_codes=True)\n",
        "#nltk\n",
        "import nltk\n",
        "\n",
        "#stop-words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# tokenizing\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "\n",
        "#import the os module to interact with the underlying operating system\n",
        "import os \n",
        "#for creating Progress Meters or Progress Bars\n",
        "from tqdm.notebook import tqdm\n",
        "# including functions to load images from files\n",
        "from PIL import Image\n",
        "#The ast. literal_eval method is one of the helper functions that helps traverse an abstract syntax tree\n",
        "from ast import literal_eval\n",
        "#split data to train and test \n",
        "from sklearn.model_selection import train_test_split\n",
        "#native Python library that allows you to customize the formatting of your output\n",
        "from pprint import pprint\n",
        "#stop words are used to eliminate unimportant words\n",
        "from nltk.corpus import stopwords\n",
        "#splits a given sentence into words using the NLTK library\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#tensorflow and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#Optimizers are used to solve optimization problems by minimizing loss\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "#Tokenization breaks the raw text into words\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "#pad_sequences is used to ensure that all sequences in a list have the same length\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#layers for neural neywork\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPool2D , Dropout , LSTM,Embedding\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#__future__ module is a built-in module in Python that is used to inherit new features that will be available in the new Python versions.\n",
        "import collections#a container that is used to store collections of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IezlK1XUj026"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stemmer = SnowballStemmer(language='english')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWZMR8W1eBcu"
      },
      "outputs": [],
      "source": [
        "#Get file of data and download it \n",
        "# ! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4.zip\n",
        "# ! unzip -q a4.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhN4X0wFeBpW"
      },
      "outputs": [],
      "source": [
        "df_train= pd.read_csv('/content/train_xy.csv')#read train data \n",
        "df_test =pd.read_csv('/content/test_x.csv' , index_col='id')#read test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "miZ2jFJaqf8m",
        "outputId": "d5b769c9-8585-41bc-851e-2d1a937d6add"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             summary            image  \\\n",
              "0  Spacious, sunny and cozy modern apartment in t...  img_train/0.jpg   \n",
              "1  Located in one of the most vibrant and accessi...  img_train/1.jpg   \n",
              "2  Logement coquet et douillet à 10 minutes du ce...  img_train/2.jpg   \n",
              "3  Beautiful and spacious (1076 sc ft, / 100 mc) ...  img_train/3.jpg   \n",
              "4  Très grand appartement ''rustique'' et très ag...  img_train/4.jpg   \n",
              "5  Small and cozy studio, all equipped, ideally s...  img_train/5.jpg   \n",
              "6  Charming apartment! A place where you will fee...  img_train/6.jpg   \n",
              "7  Grande chambre dans une maison pour étudiants....  img_train/7.jpg   \n",
              "\n",
              "        type  price  \n",
              "0  Apartment      1  \n",
              "1  Apartment      0  \n",
              "2  Apartment      1  \n",
              "3  Apartment      1  \n",
              "4  Apartment      0  \n",
              "5  Apartment      0  \n",
              "6  Apartment      0  \n",
              "7      House      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f7c0eb7-54c4-4e55-bda2-509c8d6c4ce8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet à 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Très grand appartement ''rustique'' et très ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Small and cozy studio, all equipped, ideally s...</td>\n",
              "      <td>img_train/5.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Charming apartment! A place where you will fee...</td>\n",
              "      <td>img_train/6.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Grande chambre dans une maison pour étudiants....</td>\n",
              "      <td>img_train/7.jpg</td>\n",
              "      <td>House</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f7c0eb7-54c4-4e55-bda2-509c8d6c4ce8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f7c0eb7-54c4-4e55-bda2-509c8d6c4ce8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f7c0eb7-54c4-4e55-bda2-509c8d6c4ce8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_train.head(8)#show some row of data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "hbfid_1cqwc4",
        "outputId": "b1594d76-7134-4677-aa7a-4449eafc225b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#show percent of nan values \n",
        "percentage_missing_data = df_train.isnull().sum() / df_train.shape[0]\n",
        "ax = percentage_missing_data.plot(kind = 'bar', color='#E31A5C', figsize = (10, 5))\n",
        "ax.set_xlabel('Feature')\n",
        "ax.set_ylabel('Percent Empty / NaN')\n",
        "ax.set_title('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY45TM5-blls",
        "outputId": "d5018d2c-2e2a-474c-ab7c-03fd16532e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7627 entries, 0 to 7626\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   summary  7326 non-null   object\n",
            " 1   image    7627 non-null   object\n",
            " 2   type     7627 non-null   object\n",
            " 3   price    7627 non-null   int64 \n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 238.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# information about data \n",
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5NsTOK9scY3",
        "outputId": "c7aedd8f-2b21-42e8-96f1-d0a7ea4f8555"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apartment                 5765\n",
              "Condominium                691\n",
              "House                      406\n",
              "Loft                       324\n",
              "Townhouse                  167\n",
              "Serviced apartment          77\n",
              "Bed and breakfast           38\n",
              "Guest suite                 32\n",
              "Hostel                      26\n",
              "Bungalow                    25\n",
              "Guesthouse                  14\n",
              "Cottage                     12\n",
              "Aparthotel                  12\n",
              "Boutique hotel              10\n",
              "Other                        8\n",
              "Villa                        7\n",
              "Tiny house                   3\n",
              "Boat                         2\n",
              "Cabin                        2\n",
              "Camper/RV                    2\n",
              "Casa particular (Cuba)       1\n",
              "Hotel                        1\n",
              "Earth house                  1\n",
              "Castle                       1\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#count values of our label (type)\n",
        "df_train['type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bidGd9Ny212-"
      },
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "We have image and text data.\n",
        "Image data: resize\n",
        "Text data: tokenization and converting to integer IDs\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OskuaqX0scbO"
      },
      "outputs": [],
      "source": [
        "#create function to load image from source ,resize and convert it to same size and color \n",
        "def load_image(file):\n",
        "    try:\n",
        "        image = Image.open(\n",
        "            file\n",
        "        ).convert('LA').resize((64, 64))#Represents L with Alpha transparency channel#resize((64, 64))\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        arr = np.zeros((64, 64,2))\n",
        "    return arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6e6fc2f02b244ab8b5c55bce0e2107aa",
            "091d7fae07bc49008c27d88c533712a7",
            "5aca1d0dd56c430d92ac777fe7c757e7",
            "9dc0fb7248e54925adebf1aab7bfbec1",
            "69b0315fb18d498a915d0489b8f190a0",
            "c6d4808d181b4c16afc67b1ea2bedd7f",
            "5c0c78d21ac5417594e4793ea9e0e0fd",
            "ceb6f1cdd1a34dbdaf5a085c77bddc6b",
            "a3a795998cf34b8eb21d3681c8857bef",
            "96cf89fd0939459db3cc4ae57fef2e6a",
            "6d30c9a5ebac4a79b23cebd4796dee8d"
          ]
        },
        "id": "ko9mWLYWscdi",
        "outputId": "48761a5e-527b-47a9-d258-a2ca39ed7f67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7627 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e6fc2f02b244ab8b5c55bce0e2107aa"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# loading images:\n",
        "df_train_image = np.array([load_image(i) for i in tqdm(df_train.image)])\n",
        "\n",
        "# loading overview: (force convert some of the non-string cell to string)\n",
        "df_train_text = df_train.summary.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "eepial7zscf-",
        "outputId": "fd6e1b58-5850-4bd4-89ef-54aa9da480c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# check image loading\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(df_train_image[3,:,:,0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrk0LAsv_5Wc"
      },
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D51P4xSiKKHV"
      },
      "source": [
        "## **Label**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFCO1B34AyaA"
      },
      "outputs": [],
      "source": [
        "#encode the prediction labels\n",
        "df_train['type'] = df_train['type'].astype('category').cat.codes\n",
        "df_train['price'] = df_train['price'].astype('category').cat.codes\n",
        "len_type = len(df_train['type'].unique())\n",
        "len_price = len(df_train.price.unique())\n",
        "\n",
        "# get price \n",
        "y_train_price = df_train.price\n",
        "\n",
        "# get type\n",
        "y_train_type = df_train['type']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtNCq3PJBS7P"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.dropna()#drop nan value\n",
        "df_train= df_train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thR6ucPzBqOB",
        "outputId": "4b4c5508-135c-4e57-9350-b157be3b7224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "summary    0\n",
              "image      0\n",
              "type       0\n",
              "price      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df_train.isnull().sum() #check null value for all columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThD3gBAV8yHD"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcTfy1sfHfQ8",
        "outputId": "32dd551a-2920-40a2-f76f-743d435f0a90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Spacious, sunny and cozy modern apartment in t...\n",
              "1       Located in one of the most vibrant and accessi...\n",
              "2       Logement coquet et douillet à 10 minutes du ce...\n",
              "3       Beautiful and spacious (1076 sc ft, / 100 mc) ...\n",
              "4       Très grand appartement ''rustique'' et très ag...\n",
              "                              ...                        \n",
              "7622    Un grand logement 4 et 1/2, tout inclut, bien ...\n",
              "7623    Magnificent condo directly on the river. You w...\n",
              "7624    This apartment is perfect for anyone visiting ...\n",
              "7625    It is a cozy ,clean ,and comfortable apartment...\n",
              "7626    Modern country style (newly-renovated); open c...\n",
              "Name: summary, Length: 7627, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df_train_text#show data before translate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKAPef3HW_Ot"
      },
      "outputs": [],
      "source": [
        "translator = Translator()#show data before translate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1UsFWqqVpH0"
      },
      "outputs": [],
      "source": [
        "df_train_text= df_train_text.apply(lambda x: translator.translate(x, dest='en').text)#the data has multi language so i will translate it to english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mzuIVDMoJIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc93063b-f51c-4149-a1f6-58ef5f4ed2e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Spacious, sunny and cozy modern apartment in t...\n",
              "1       Located in one of the most vibrant and accessi...\n",
              "2       Pretty and cozy accommodation 10 minutes from ...\n",
              "3       Beautiful and spacious (1076 sc ft, / 100 mc) ...\n",
              "4       Very large ''rustic'' and very pleasant apartm...\n",
              "                              ...                        \n",
              "7622    A large 4 and 1/2 apartment, all inclusive, we...\n",
              "7623    Magnificent condo directly on the river. You w...\n",
              "7624    This apartment is perfect for anyone visiting ...\n",
              "7625    It is a cozy ,clean ,and comfortable apartment...\n",
              "7626    Modern country style (newly-renovated); open c...\n",
              "Name: summary, Length: 7627, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df_train_text#show data after translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9znp-Ltvi4UB"
      },
      "outputs": [],
      "source": [
        "#create function to clean data and remove stop words\n",
        "def clean_text(text, for_embedding=False):\n",
        "\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) #ignore white space \n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")#remove tags\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)#remove ascii code\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)#remove singe char\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dS_yTm2i4NK"
      },
      "outputs": [],
      "source": [
        "df_train_text= df_train_text.apply(clean_text)#apply function on text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLULBjhdoO2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b920a830-7510-455a-d387-b323d74845c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       spacious sunni cozi modern apart heart montrea...\n",
              "1       locat one vibrant access locat downtown montre...\n",
              "2       pretti cozi accommod minut downtown montreal g...\n",
              "3       beauti spacious sc ft mc condo th floor west i...\n",
              "4       larg rustic pleasant apart rent nice neighborh...\n",
              "                              ...                        \n",
              "7622    larg apart inclus well lit locat quiet build r...\n",
              "7623    magnific condo direct river sleep song wild bi...\n",
              "7624    apart perfect anyon visit citi locat eclect ne...\n",
              "7625    cozi clean comfort apart heart downtown websit...\n",
              "7626    modern countri style newli renov open concept ...\n",
              "Name: summary, Length: 7627, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df_train_text#show data after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN2brs58X_WO"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "DataFrame(df_train_text).to_csv(\"edited.csv\")#save the editing into csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P-lo06bCCT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbcfd0c-dda9-4f94-a9eb-1514a37ae2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras_preprocessing.text.Tokenizer object at 0x7f24e62a4c90>\n",
            "(7627, 100)\n"
          ]
        }
      ],
      "source": [
        "# preprocess text data\n",
        "#Updates internal vocabulary based on a list of texts.\n",
        "# In the case where texts contains lists, we assume each entry of the lists to be a token.\n",
        "# Required before using texts_to_sequences or texts_to_matrix\n",
        "vocab_size = 600000#40000\n",
        "max_len = 100\n",
        "\n",
        "# build vocabulary from training set\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "print(tokenizer )\n",
        "tokenizer.fit_on_texts(df_train_text)\n",
        "\n",
        "#The Keras Embedding layer requires all individual documents to be of same length\n",
        "#To pad the shorter documents I am using pad_sequences functon from the Keras library.\n",
        "def _preprocess(list_of_text):\n",
        "    return pad_sequences(\n",
        "        #Transforms each text in texts to a sequence of integers.\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        maxlen=max_len,\n",
        "        #Optional Int, maximum length of all sequences. If not provided, sequences\n",
        "        # will be padded to the length of the longest individual sequence.\n",
        "        # pad variable length sequences with dummy values\n",
        "        #String, 'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence.\n",
        "        padding='post',\n",
        "    )\n",
        "\n",
        "# padding is done inside: \n",
        "df_train_text_id = _preprocess(df_train_text)\n",
        "\n",
        "print(df_train_text_id.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8w-3ITfCvVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4959d0b-2d50-474f-d7fd-4c82ea6b5b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spacious sunni cozi modern apart heart montreal bedroom central locat '\n",
            " 'popular plateau mont royal neighborhood middl princ arthur pedestrian street '\n",
            " 'close amen restaur coffe hous bar club shop univers subway station experi '\n",
            " 'montreal like real local resid heart action grand prix week grill saint '\n",
            " 'laurent festiv mural much',\n",
            " 'locat one vibrant access locat downtown montreal one bedroom condo impress '\n",
            " 'leav one memor experi walk distanc popular saint catherin street bell center '\n",
            " 'old port lachin canal bonaventur metro much much',\n",
            " 'pretti cozi accommod minut downtown montreal groceri store pharmaci saq '\n",
            " 'restaur public transport nearbi two close bedroom accommod adult']\n"
          ]
        }
      ],
      "source": [
        "# we can use the tokenizer to convert IDs to words.by using \"sequences_to_texts\" \n",
        "pprint(tokenizer.sequences_to_texts(df_train_text_id[:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b_2ecq3DXI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4567bda9-7719-4163-c6c7-14c15976751b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words in the dictionary: 600000\n"
          ]
        }
      ],
      "source": [
        "#print the number of words in the dictionary \n",
        "print('total words in the dictionary:', tokenizer.num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hdIyJvL4UKT"
      },
      "source": [
        "\n",
        "\n",
        "`\n",
        "Thus the embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space.\n",
        "`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTCOXElHEuNc"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "PARAMETERS OF THE EMBEDDING LAYER \n",
        "'input_dim' = the vocab size that we will choose.\n",
        "In other words it is the number of unique words in the vocab.\n",
        "'output_dim' = the number of dimensions we wish to embed into.\n",
        "Each word will be represented by a vector of this much dimensions.\n",
        "'input_length' = lenght of the maximum document. which is stored in maxlen variable in our case\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trail_1**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2U7-_eX5k00j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cover dropout layer"
      ],
      "metadata": {
        "id": "buh5CG8Gze4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Plan_1:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "we have input image,i will use conv2d as first layer then i will use dropout \n",
        "and maxpolling is second and third layer then make flatten \n",
        "the output predict multilabel so i will use two dense layer with \n",
        "activation function softmax \n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "alDOHMCZzbUX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It87wd7_DXnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029c4e38-2c52-43b2-db3c-a828c12c2c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 50, 50, 10)   4510        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 50, 50, 10)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     60000000    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 5, 5, 10)     0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 250)          0           ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 350)          0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1053        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           8424        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,013,987\n",
            "Trainable params: 60,013,987\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "\n",
        "# simple conv2d with dropout\n",
        "cov1 = Conv2D(10, (15,15) , activation='relu')(in_image) # 10 number of filters  and  (15, 15) size of filter\n",
        "con_drop = Dropout(0.2)(cov1) #add drop out layer \n",
        "pl = MaxPool2D((10, 10))(con_drop)#add maxpooling layer \n",
        "flattened = Flatten()(pl)#add flatten layer \n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)#merge between two input \n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)#layer for predict type \n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)#layer for predict price \n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),#using adam optimizer with learning rate 1e-3\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.2,#becouse i need to focus on price i put type to 0.2to make model concentrate on one label \n",
        "        'price': 0.7,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAyGx2lTDXqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79127e52-2232-4608-8864-bf265484ed0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "382/382 [==============================] - 25s 60ms/step - loss: 1.3108 - price_loss: 1.2982 - type_loss: 2.0103 - price_sparse_categorical_accuracy: 0.5999 - type_sparse_categorical_accuracy: 0.7335 - val_loss: 0.7652 - val_price_loss: 0.8141 - val_type_loss: 0.9766 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/10\n",
            "382/382 [==============================] - 23s 60ms/step - loss: 0.7640 - price_loss: 0.8054 - type_loss: 1.0013 - price_sparse_categorical_accuracy: 0.6224 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.7415 - val_price_loss: 0.7864 - val_type_loss: 0.9550 - val_price_sparse_categorical_accuracy: 0.6343 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/10\n",
            "382/382 [==============================] - 23s 59ms/step - loss: 0.7328 - price_loss: 0.7668 - type_loss: 0.9804 - price_sparse_categorical_accuracy: 0.6415 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.7188 - val_price_loss: 0.7583 - val_type_loss: 0.9400 - val_price_sparse_categorical_accuracy: 0.6468 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 4/10\n",
            "382/382 [==============================] - 23s 59ms/step - loss: 0.7016 - price_loss: 0.7274 - type_loss: 0.9622 - price_sparse_categorical_accuracy: 0.6728 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.6950 - val_price_loss: 0.7286 - val_type_loss: 0.9250 - val_price_sparse_categorical_accuracy: 0.6717 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 5/10\n",
            "382/382 [==============================] - 22s 59ms/step - loss: 0.6759 - price_loss: 0.6960 - type_loss: 0.9434 - price_sparse_categorical_accuracy: 0.6968 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.6824 - val_price_loss: 0.7139 - val_type_loss: 0.9131 - val_price_sparse_categorical_accuracy: 0.6769 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 6/10\n",
            "382/382 [==============================] - 23s 59ms/step - loss: 0.6544 - price_loss: 0.6699 - type_loss: 0.9275 - price_sparse_categorical_accuracy: 0.7132 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.6734 - val_price_loss: 0.7047 - val_type_loss: 0.9004 - val_price_sparse_categorical_accuracy: 0.6756 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 7/10\n",
            "382/382 [==============================] - 23s 60ms/step - loss: 0.6367 - price_loss: 0.6492 - type_loss: 0.9112 - price_sparse_categorical_accuracy: 0.7246 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.6686 - val_price_loss: 0.7005 - val_type_loss: 0.8916 - val_price_sparse_categorical_accuracy: 0.6776 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 8/10\n",
            "382/382 [==============================] - 23s 60ms/step - loss: 0.6194 - price_loss: 0.6291 - type_loss: 0.8949 - price_sparse_categorical_accuracy: 0.7358 - type_sparse_categorical_accuracy: 0.7543 - val_loss: 0.6646 - val_price_loss: 0.6980 - val_type_loss: 0.8800 - val_price_sparse_categorical_accuracy: 0.6855 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 9/10\n",
            "382/382 [==============================] - 23s 59ms/step - loss: 0.6037 - price_loss: 0.6112 - type_loss: 0.8792 - price_sparse_categorical_accuracy: 0.7441 - type_sparse_categorical_accuracy: 0.7543 - val_loss: 0.6639 - val_price_loss: 0.6992 - val_type_loss: 0.8725 - val_price_sparse_categorical_accuracy: 0.6802 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 10/10\n",
            "382/382 [==============================] - 23s 59ms/step - loss: 0.5890 - price_loss: 0.5952 - type_loss: 0.8619 - price_sparse_categorical_accuracy: 0.7514 - type_sparse_categorical_accuracy: 0.7546 - val_loss: 0.6623 - val_price_loss: 0.7005 - val_type_loss: 0.8597 - val_price_sparse_categorical_accuracy: 0.6841 - val_type_sparse_categorical_accuracy: 0.7654\n"
          ]
        }
      ],
      "source": [
        "# set seed to reproduce results\n",
        "seed = 2021\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puOKjt9x7Ld7"
      },
      "outputs": [],
      "source": [
        "df_test_text = df_test.summary.astype('str')#force test summary to be string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhPiUIZ85xSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a407715f-60a1-45e9-ffef-6f65af86b888"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0       Charming warm house is ready to host you here ...\n",
              "1       La chambre est spacieuse et lumineuse, dans un...\n",
              "2       Grande chambre confortable située au sous-sol ...\n",
              "3       Près d’un Métro, ligne orange. 10 minutes à pi...\n",
              "4       Very bright appartment and very cosy. 2 separa...\n",
              "                              ...                        \n",
              "7626    Large, fully-furnished flat with brick walls a...\n",
              "7627    Logement situé dans le haut d’un duplex. Vivez...\n",
              "7628    My place is close to parks, . My place is good...\n",
              "7629    *** For security reasons, I will prioritize gu...\n",
              "7630    Stay in an amazing area of Montreal! 5-7 min f...\n",
              "Name: summary, Length: 7360, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_test_text#read before any preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yxYAZsbiVPO"
      },
      "outputs": [],
      "source": [
        "df_test_text=df_test_text.apply(lambda x: translator.translate(x, dest='en').text)#translate test sammary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SK1S5mr6Z5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4df6b8-ccd5-4570-96eb-8cfcfbb8c5f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0       Charming warm house is ready to host you here ...\n",
              "1       The room is spacious and bright, in an apartme...\n",
              "2       Large comfortable room located in the basement...\n",
              "3       Near a Metro, orange line. 10 minutes walking....\n",
              "4       Very bright appartment and very cosy. 2 separa...\n",
              "                              ...                        \n",
              "7626    Large, fully-furnished flat with brick walls a...\n",
              "7627    Accommodation located at the top of a duplex. ...\n",
              "7628    My place is close to parks, . My place is good...\n",
              "7629    *** For security reasons, I will prioritize gu...\n",
              "7630    Stay in an amazing area of Montreal! 5-7 min f...\n",
              "Name: summary, Length: 7360, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df_test_text#read data after translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jssbmZSWpF2C"
      },
      "outputs": [],
      "source": [
        "df_test_text= df_test_text.apply(clean_text)#apply clean function to text in test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftGoZtdcECZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "46e128804ea04207b60adf44ccd4f068",
            "9137cd0ff745423281834d7b249b39f5",
            "8be5a3b58e5b449d9da904fdcac2bc5c",
            "3a36a12cf9634c68ab13e0b02ad4ef2f",
            "3fe331b6f6754d6b9808ce3b7fc89cc7",
            "e4d219b170ad4859bb43a35db17f62b2",
            "7e24d849d7e24a7db919522b0beeb710",
            "13afae4fbbe447609930cd4a4f6a008d",
            "bcf97e9dd33e4797b3ae125f338eded3",
            "974c577e36894c859d58d6a5a8a1e204",
            "feebde0b29b64158aed238203bfdc7db"
          ]
        },
        "outputId": "c7f3c800-0264-4fb5-e2f3-eeb525f4eb63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e128804ea04207b60adf44ccd4f068"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# loading images:\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(df_test.image)])\n",
        "\n",
        "# loading overview: (force convert some of the non-string cell to string) and apply preprocessing\n",
        "\n",
        "x_test_text = _preprocess(df_test_text.astype('str'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwuGB9mwEIcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f4355c-429f-41e3-9fc0-5bd5649e26ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.69577837 0.24337184 0.0608498 ]\n",
            " [0.9434312  0.04737906 0.00918977]\n",
            " [0.8349234  0.12240695 0.04266969]\n",
            " ...\n",
            " [0.80028254 0.17055294 0.02916448]\n",
            " [0.9699008  0.02294466 0.00715455]\n",
            " [0.7471453  0.22140555 0.03144915]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price'] #predict one target from multi objective \n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ6MBC9LEMng"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about price prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission_trail1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation_1**\n",
        "\n",
        "```\n",
        "-Dropout regularization is a computationally cheap way \n",
        "to regularize a deep neural network.\n",
        "\n",
        "-Dropout is created as a regularization technique, \n",
        "that we can use to reduce the model capacity so that \n",
        "our model can achieve lower generalization error\n",
        "\n",
        "-Dropout: A Simple Way to Prevent Neural Networks from Overfitting \n",
        "Dropout has the effect of making the training process noisy\n",
        "forcing nodes within a layer to probabilistically take on more \n",
        "or less responsibility for the inputs\n",
        "\n",
        "\n",
        "in this trail I used (epochs=10,batch_size=16)\n",
        "then i get results \n",
        "Private score :0.69456\n",
        "the public score : 0.68043\n",
        "\n",
        "```\n",
        "\n",
        " \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "M3y-WQ3rqV00"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnzv2Jj9-iL"
      },
      "source": [
        "## **Trail_2**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cover Conv2d layer "
      ],
      "metadata": {
        "id": "kKrZK0N0zOxA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNNGnftg-Hiv"
      },
      "source": [
        "\n",
        "```\n",
        "Plan_2:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "we have input image,i will I will Cover Conv2d layer  \n",
        "and maxpolling is second and third layer then make flatten \n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax \n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOdl4i3VENmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f62845-1864-42db-d9f7-0fccda759998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_16 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 50, 50, 15)   6765        ['input_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 41, 41, 10)   15010       ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 27, 27, 15)   33765       ['conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, 100, 100)     60000000    ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 9, 9, 15)    0           ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_7 (TFOpLam  (None, 100)         0           ['embedding_7[0][0]']            \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_7 (Flatten)            (None, 1215)         0           ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)       (None, 1315)         0           ['tf.math.reduce_mean_7[0][0]',  \n",
            "                                                                  'flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            3948        ['tf.concat_5[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           31584       ['tf.concat_5[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,091,072\n",
            "Trainable params: 60,091,072\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d\n",
        "cov1 =layers.Conv2D(15, 15 ,activation='relu')(in_image)\n",
        "cov2 =layers.Conv2D(10, 10,activation='relu')(cov1)\n",
        "cov3=layers.Conv2D(15, 15,activation='relu')(cov2)\n",
        "pl = MaxPool2D((3, 3))(cov3)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model2 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model2.compile(\n",
        "    optimizer=Adam(0.001),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0,\n",
        "        'price': 1.0,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZmefau5ENo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a331cb9a-1bab-4ea5-9c6b-4f73bb7dc213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "226/226 [==============================] - 18s 73ms/step - loss: 1.7270 - price_loss: 1.7270 - type_loss: 5.8099 - price_sparse_categorical_accuracy: 0.6143 - type_sparse_categorical_accuracy: 0.0602 - val_loss: 0.8181 - val_price_loss: 0.8181 - val_type_loss: 3.1664 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0341\n",
            "Epoch 2/8\n",
            "226/226 [==============================] - 16s 72ms/step - loss: 0.8117 - price_loss: 0.8117 - type_loss: 3.1640 - price_sparse_categorical_accuracy: 0.6212 - type_sparse_categorical_accuracy: 0.0415 - val_loss: 0.7926 - val_price_loss: 0.7926 - val_type_loss: 3.1618 - val_price_sparse_categorical_accuracy: 0.6330 - val_type_sparse_categorical_accuracy: 0.0570\n",
            "Epoch 3/8\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.7725 - price_loss: 0.7725 - type_loss: 3.1607 - price_sparse_categorical_accuracy: 0.6374 - type_sparse_categorical_accuracy: 0.0561 - val_loss: 0.7593 - val_price_loss: 0.7593 - val_type_loss: 3.1586 - val_price_sparse_categorical_accuracy: 0.6474 - val_type_sparse_categorical_accuracy: 0.0616\n",
            "Epoch 4/8\n",
            "226/226 [==============================] - 16s 70ms/step - loss: 0.7297 - price_loss: 0.7297 - type_loss: 3.1616 - price_sparse_categorical_accuracy: 0.6692 - type_sparse_categorical_accuracy: 0.0644 - val_loss: 0.7299 - val_price_loss: 0.7299 - val_type_loss: 3.1617 - val_price_sparse_categorical_accuracy: 0.6730 - val_type_sparse_categorical_accuracy: 0.0754\n",
            "Epoch 5/8\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.6957 - price_loss: 0.6957 - type_loss: 3.1628 - price_sparse_categorical_accuracy: 0.6946 - type_sparse_categorical_accuracy: 0.0695 - val_loss: 0.7147 - val_price_loss: 0.7147 - val_type_loss: 3.1634 - val_price_sparse_categorical_accuracy: 0.6809 - val_type_sparse_categorical_accuracy: 0.0786\n",
            "Epoch 6/8\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.6688 - price_loss: 0.6688 - type_loss: 3.1637 - price_sparse_categorical_accuracy: 0.7127 - type_sparse_categorical_accuracy: 0.0705 - val_loss: 0.7052 - val_price_loss: 0.7052 - val_type_loss: 3.1643 - val_price_sparse_categorical_accuracy: 0.6776 - val_type_sparse_categorical_accuracy: 0.0799\n",
            "Epoch 7/8\n",
            "226/226 [==============================] - 16s 70ms/step - loss: 0.6467 - price_loss: 0.6467 - type_loss: 3.1643 - price_sparse_categorical_accuracy: 0.7223 - type_sparse_categorical_accuracy: 0.0711 - val_loss: 0.7009 - val_price_loss: 0.7009 - val_type_loss: 3.1645 - val_price_sparse_categorical_accuracy: 0.6815 - val_type_sparse_categorical_accuracy: 0.0806\n",
            "Epoch 8/8\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.6264 - price_loss: 0.6264 - type_loss: 3.1650 - price_sparse_categorical_accuracy: 0.7332 - type_sparse_categorical_accuracy: 0.0728 - val_loss: 0.6975 - val_price_loss: 0.6975 - val_type_loss: 3.1653 - val_price_sparse_categorical_accuracy: 0.6881 - val_type_sparse_categorical_accuracy: 0.0839\n"
          ]
        }
      ],
      "source": [
        "history = model2.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=8,\n",
        "    batch_size=27,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=10, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6utyTWENrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2c75c5-502a-44ae-8968-abafec0cbb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.67853063 0.26944304 0.05202628]\n",
            " [0.8967946  0.08712506 0.01608035]\n",
            " [0.76633507 0.19103633 0.04262853]\n",
            " ...\n",
            " [0.75141144 0.21489374 0.0336948 ]\n",
            " [0.94350207 0.04657099 0.00992702]\n",
            " [0.6884791  0.2747543  0.03676664]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model2.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIDtGCbk-gVQ"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('trail_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation_2**\n",
        "\n",
        "```\n",
        "-I used three layer conv2d and one maxpooling \n",
        "\n",
        "2D convolution layer (e.g. spatial convolution over images).\n",
        "\n",
        "This layer creates a convolution kernel that is convolved with the layer\n",
        " input to produce a tensor of outputs. If use_bias is True, a bias \n",
        " vector is created and added to the outputs. Finally, \n",
        " if activation is not None, it is applied to the outputs as well.\n",
        "\n",
        "When using this layer as the first layer in a model, provide \n",
        "the keyword argument input_shape (tuple of integers or None, \n",
        "does not include the sample axis), e.g. input_shape=(128, 128, 3) \n",
        "for 128x128 RGB pictures in data_format=\"channels_last\". \n",
        "\n",
        "\n",
        "-I used batch size 27 and epoch 8\n",
        "then i got result :\n",
        "public score : 0.68043\n",
        "Private score :0.69456\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AuIn1UucTDnE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsGlnwU8Frni"
      },
      "source": [
        "## **Trail_3**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cover GRU layer \n",
        "\n",
        "```\n",
        "Plan_3:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "\n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "\n",
        "we have input image,i will I will Cover Conv2d layer  \n",
        "and maxpolling is second and third layer then make flatten \n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax \n",
        "\n",
        "and i will add GRU layer for text input \n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "s2EF2ISwTkP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YioBlomr-kyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e58557d-cbc7-4475-ccc4-33b57e41fdb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_28 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_27 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 50, 50, 10)   4510        ['input_28[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)       (None, 100, 100)     60000000    ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooling2D  (None, 10, 10, 10)  0           ['conv2d_26[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " gru_5 (GRU)                    (None, 4)            1272        ['embedding_13[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_13 (Flatten)           (None, 1000)         0           ['max_pooling2d_11[0][0]']       \n",
            "                                                                                                  \n",
            " tf.concat_11 (TFOpLambda)      (None, 1004)         0           ['gru_5[0][0]',                  \n",
            "                                                                  'flatten_13[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            3015        ['tf.concat_11[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           24120       ['tf.concat_11[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,032,917\n",
            "Trainable params: 60,032,917\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "gru =keras.layers.GRU(4)(embedded)\n",
        "\n",
        "# image part \n",
        "# simple conv2d with dropout\n",
        "cov1 = Conv2D(10, (15,15) , activation='relu')(in_image) # 10 number of filters  and  (15, 15) size of filter\n",
        "pl = MaxPool2D((5, 5))(cov1)#add maxpooling layer \n",
        "flattened = Flatten()(pl)#add flatten layer \n",
        "# fusion - combinig both\n",
        "fused = tf.concat([gru, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model3= keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model3.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.0,\n",
        "        'price': 1.0,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nzShbpp-k0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ecb587-8226-4403-e7d7-5077fb7807f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - 19s 74ms/step - loss: 4.0864 - price_loss: 4.0864 - type_loss: 10.8058 - price_sparse_categorical_accuracy: 0.6084 - type_sparse_categorical_accuracy: 0.0043 - val_loss: 0.8350 - val_price_loss: 0.8350 - val_type_loss: 3.1940 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8380 - price_loss: 0.8380 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8303 - val_price_loss: 0.8303 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8366 - price_loss: 0.8366 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8303 - val_price_loss: 0.8303 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8365 - price_loss: 0.8365 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8301 - val_price_loss: 0.8301 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8364 - price_loss: 0.8364 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8302 - val_price_loss: 0.8302 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - 16s 72ms/step - loss: 0.8364 - price_loss: 0.8364 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8300 - val_price_loss: 0.8300 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8364 - price_loss: 0.8364 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8305 - val_price_loss: 0.8305 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - 16s 70ms/step - loss: 0.8363 - price_loss: 0.8363 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8302 - val_price_loss: 0.8302 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8364 - price_loss: 0.8364 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8300 - val_price_loss: 0.8300 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - 16s 71ms/step - loss: 0.8365 - price_loss: 0.8365 - type_loss: 3.1941 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.0020 - val_loss: 0.8300 - val_price_loss: 0.8300 - val_type_loss: 3.1939 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n"
          ]
        }
      ],
      "source": [
        "history = model3.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=10,\n",
        "    batch_size=27,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=10, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S2wzJeQ-k3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df7346e-1de2-4584-bd67-af20b2ea315d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.62225384 0.3135862  0.0641599 ]\n",
            " [0.62225384 0.3135862  0.0641599 ]\n",
            " [0.62225384 0.3135862  0.0641599 ]\n",
            " ...\n",
            " [0.62225384 0.3135862  0.0641599 ]\n",
            " [0.62225384 0.3135862  0.0641599 ]\n",
            " [0.62225384 0.3135862  0.0641599 ]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model3.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKs18BHV-k5k"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submissiontrail3.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation_3**\n",
        "\n",
        "```\n",
        "-Gated Recurrent Unit\n",
        "Based on available runtime hardware and constraints, \n",
        "this layer will choose different implementations \n",
        "(cuDNN-based or pure-TensorFlow) to maximize the performance. \n",
        "If a GPU is available and all the arguments to the layer meet \n",
        "the requirement of the cuDNN kernel  \n",
        "the layer will use a fast cuDNN implementation.\n",
        "\n",
        "-This layer is a simple fully connected layer with Gated recurrent \n",
        "units instead of simple neurons. Gated recurrent units (GRUs) \n",
        "are improved version of standard recurrent neural network. \n",
        "The GRU is like a long short-term memory (LSTM) but with \n",
        "fewer parameters. This is really useful when predicting \n",
        "time series or classifying sequential data.\n",
        "\n",
        "I used epochs=10,batch_size=27 then i notice when i added gru \n",
        "layer the accuracy was fixed over all epochs \n",
        "I got results is \n",
        "Score: 0.62934\n",
        "Public score: 0.62038\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Ap45v2lMeNJK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u9kpbNrKi-u"
      },
      "source": [
        "# **Trail_4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKNulTTzK6zp"
      },
      "source": [
        "\n",
        "Cover LSTM layer \n",
        "\n",
        "```\n",
        "\n",
        "Plan_4:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "\n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "\n",
        "we have input image,i will I will Cover Conv2d layer  \n",
        "and maxpolling is second and third layer then make flatten \n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax \n",
        "\n",
        "I will add LSTM layer to input text\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQB3kvke-k9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abcbe762-626a-4b3e-e60f-48359226da4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_30 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 49, 49, 32)   16416       ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " input_29 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 49, 49, 32)   0           ['conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_14 (Embedding)       (None, 100, 100)     60000000    ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 9, 9, 32)    0           ['dropout_4[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 4)            1680        ['embedding_14[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_14 (Flatten)           (None, 2592)         0           ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " tf.concat_12 (TFOpLambda)      (None, 2596)         0           ['lstm[0][0]',                   \n",
            "                                                                  'flatten_14[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            7791        ['tf.concat_12[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           62328       ['tf.concat_12[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,088,215\n",
            "Trainable params: 60,088,215\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "lstm = tf.keras.layers.LSTM(4)(embedded)\n",
        "# averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "# Conv2D(10, (15, 15) , activation='relu') \n",
        "cov1 = Conv2D(32, (16, 16))(in_image) # 32 number of filters  and  (16, 16) size of filter\n",
        "con_drop = Dropout(0.2)(cov1)\n",
        "# cov2= Conv2D(10, (16, 16))(con_drop)\n",
        "pl = MaxPool2D((5, 5))(con_drop)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([lstm, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model4 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model4.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0,\n",
        "        'price': 1.0,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK_C5ELgLPpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f437ec-95db-4456-a6e0-5c7de4b629eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "339/339 [==============================] - 29s 75ms/step - loss: 25.0661 - price_loss: 25.0661 - type_loss: 69.9554 - price_sparse_categorical_accuracy: 0.4547 - type_sparse_categorical_accuracy: 0.0552 - val_loss: 764.5562 - val_price_loss: 764.5562 - val_type_loss: 914.2404 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0066\n",
            "Epoch 2/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 18.2701 - price_loss: 18.2701 - type_loss: 70.6788 - price_sparse_categorical_accuracy: 0.4622 - type_sparse_categorical_accuracy: 0.0700 - val_loss: 1184.5801 - val_price_loss: 1184.5801 - val_type_loss: 1425.0093 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 3/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 16.2253 - price_loss: 16.2253 - type_loss: 81.2993 - price_sparse_categorical_accuracy: 0.4784 - type_sparse_categorical_accuracy: 0.0425 - val_loss: 1092.0708 - val_price_loss: 1092.0708 - val_type_loss: 1260.9259 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0066\n",
            "Epoch 4/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 14.1900 - price_loss: 14.1900 - type_loss: 69.3434 - price_sparse_categorical_accuracy: 0.4701 - type_sparse_categorical_accuracy: 0.0410 - val_loss: 741.1815 - val_price_loss: 741.1815 - val_type_loss: 1187.4498 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0066\n",
            "Epoch 5/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 20.4438 - price_loss: 20.4438 - type_loss: 96.9071 - price_sparse_categorical_accuracy: 0.4657 - type_sparse_categorical_accuracy: 0.0370 - val_loss: 1745.3616 - val_price_loss: 1745.3616 - val_type_loss: 1615.5076 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 6/20\n",
            "339/339 [==============================] - 25s 73ms/step - loss: 19.0738 - price_loss: 19.0738 - type_loss: 95.4233 - price_sparse_categorical_accuracy: 0.4796 - type_sparse_categorical_accuracy: 0.0434 - val_loss: 1286.1270 - val_price_loss: 1286.1270 - val_type_loss: 1835.2655 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 7/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 18.1659 - price_loss: 18.1659 - type_loss: 88.8216 - price_sparse_categorical_accuracy: 0.4778 - type_sparse_categorical_accuracy: 0.0406 - val_loss: 1982.3036 - val_price_loss: 1982.3036 - val_type_loss: 1845.3270 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0079\n",
            "Epoch 8/20\n",
            "339/339 [==============================] - 24s 71ms/step - loss: 21.5013 - price_loss: 21.5013 - type_loss: 98.4704 - price_sparse_categorical_accuracy: 0.4698 - type_sparse_categorical_accuracy: 0.0425 - val_loss: 2116.8813 - val_price_loss: 2116.8813 - val_type_loss: 2018.9249 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 9/20\n",
            "339/339 [==============================] - 25s 73ms/step - loss: 19.3637 - price_loss: 19.3637 - type_loss: 89.4955 - price_sparse_categorical_accuracy: 0.4965 - type_sparse_categorical_accuracy: 0.0410 - val_loss: 1574.0044 - val_price_loss: 1574.0044 - val_type_loss: 2035.4969 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 10/20\n",
            "339/339 [==============================] - 25s 73ms/step - loss: 25.6083 - price_loss: 25.6083 - type_loss: 114.9478 - price_sparse_categorical_accuracy: 0.4842 - type_sparse_categorical_accuracy: 0.0433 - val_loss: 2146.0476 - val_price_loss: 2146.0476 - val_type_loss: 1681.7773 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "history = model4.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=18,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmI0KC3vLZTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2202a43a-64f4-4a81-bab1-dbeaff68d997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model4.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLTigAIHLfOf"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submissiontrail4.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation_4**\n",
        "\n",
        "```\n",
        "I covered LstM layer\n",
        "\n",
        "LSTMs are explicitly designed to avoid the long-term \n",
        "dependency problem. Remembering information for long \n",
        "periods of time is practically their default behavior, \n",
        "not something they struggle to learn!\n",
        "\n",
        "An LSTM layer learns long-term dependencies between \n",
        "time steps in time series and sequence data.\n",
        "\n",
        "The layer performs additive interactions, which can help \n",
        "improve gradient flow over long sequences during training\n",
        "\n",
        "why accuracy don't change ?\n",
        "The most likely reason is that the optimizer is not suited \n",
        "to your dataset.\n",
        "\n",
        "I got the same result of trail3 \n",
        "Private Score: 0.62934\n",
        "Public score: 0.62038\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sMXWvburifNU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWAMhpIDLkGH"
      },
      "source": [
        "# **Trail5-Bonus**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Plan_5:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "\n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "\n",
        "we have input image,i will I will Cover Conv2d layer  \n",
        "and will use vgg19 model (transfer learning) then add flatten \n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax \n",
        "```"
      ],
      "metadata": {
        "id": "cQuzdFnQ030_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbnJKXxOLoKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9006e6e-20db-4e32-e116-5df9ef36cd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 51, 51, 10)   3930        ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_18 (Embedding)       (None, 100, 100)     60000000    ['input_40[0][0]']               \n",
            "                                                                                                  \n",
            " vgg19 (Functional)             (None, 1, 1, 512)    20028416    ['conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_12 (TFOpLa  (None, 100)         0           ['embedding_18[0][0]']           \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_18 (Flatten)           (None, 512)          0           ['vgg19[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.concat_15 (TFOpLambda)      (None, 612)          0           ['tf.math.reduce_mean_12[0][0]', \n",
            "                                                                  'flatten_18[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1839        ['tf.concat_15[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           14712       ['tf.concat_15[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 80,048,897\n",
            "Trainable params: 80,048,897\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding.\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d and vgg 19 \n",
        "cov1 = Conv2D(10,(14,14), activation='softmax')(in_image) # 10 number of filters  and  (15, 15) size of filter\n",
        "vgg=VGG19(weights=None, input_shape=( 51, 51, 10), include_top=False)(cov1)\n",
        "flattened= Flatten()(vgg)\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model5= keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model5.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0,\n",
        "        'price': 2.0,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0m_L5pjPye2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c022d0-00f1-4551-9392-cb1198af9258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "359/359 [==============================] - 62s 162ms/step - loss: 4.0224 - price_loss: 2.0112 - type_loss: 3.3263 - price_sparse_categorical_accuracy: 0.6166 - type_sparse_categorical_accuracy: 0.0085 - val_loss: 1.6079 - val_price_loss: 0.8040 - val_type_loss: 3.1773 - val_price_sparse_categorical_accuracy: 0.6317 - val_type_sparse_categorical_accuracy: 0.0039\n",
            "Epoch 2/5\n",
            "359/359 [==============================] - 56s 157ms/step - loss: 1.5530 - price_loss: 0.7765 - type_loss: 3.1815 - price_sparse_categorical_accuracy: 0.6386 - type_sparse_categorical_accuracy: 0.0038 - val_loss: 1.4975 - val_price_loss: 0.7488 - val_type_loss: 3.1966 - val_price_sparse_categorical_accuracy: 0.6619 - val_type_sparse_categorical_accuracy: 0.0046\n",
            "Epoch 3/5\n",
            "359/359 [==============================] - 56s 157ms/step - loss: 1.4312 - price_loss: 0.7156 - type_loss: 3.1853 - price_sparse_categorical_accuracy: 0.6896 - type_sparse_categorical_accuracy: 0.0038 - val_loss: 1.4586 - val_price_loss: 0.7293 - val_type_loss: 3.1746 - val_price_sparse_categorical_accuracy: 0.6697 - val_type_sparse_categorical_accuracy: 0.0046\n",
            "Epoch 4/5\n",
            "359/359 [==============================] - 56s 157ms/step - loss: 1.3514 - price_loss: 0.6757 - type_loss: 3.1623 - price_sparse_categorical_accuracy: 0.7099 - type_sparse_categorical_accuracy: 0.0033 - val_loss: 1.4103 - val_price_loss: 0.7052 - val_type_loss: 3.1606 - val_price_sparse_categorical_accuracy: 0.6776 - val_type_sparse_categorical_accuracy: 0.0039\n",
            "Epoch 5/5\n",
            "359/359 [==============================] - 56s 157ms/step - loss: 1.2881 - price_loss: 0.6440 - type_loss: 3.1704 - price_sparse_categorical_accuracy: 0.7287 - type_sparse_categorical_accuracy: 0.0033 - val_loss: 1.4028 - val_price_loss: 0.7014 - val_type_loss: 3.1750 - val_price_sparse_categorical_accuracy: 0.6809 - val_type_sparse_categorical_accuracy: 0.0039\n"
          ]
        }
      ],
      "source": [
        "history = model5.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,\n",
        "    batch_size=17,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih8dmvhUP6y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c6e70b-8598-45ec-b4a3-5cb79bf98045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6649983  0.2854401  0.0495616 ]\n",
            " [0.8971545  0.08767181 0.01517378]\n",
            " [0.7674255  0.19469301 0.03788149]\n",
            " ...\n",
            " [0.74189633 0.22600438 0.03209926]\n",
            " [0.94226676 0.04717998 0.01055324]\n",
            " [0.6667539  0.29204533 0.04120075]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model5.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7yOKpT1QCxD"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission5.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation5**\n",
        "\n",
        "\n",
        "```\n",
        "Visual Geometry Group (vgg)uses deep Convolutional neural layers to improve accuracy.\n",
        "\n",
        "So in simple language VGG is a deep CNN used to classify images. \n",
        "The layers in VGG19 model are as follows:\n",
        "\n",
        "Conv3x3 (64)\n",
        "Conv3x3 (64)\n",
        "MaxPool\n",
        "Conv3x3 (128)\n",
        "Conv3x3 (128)\n",
        "MaxPool\n",
        "Conv3x3 (256)\n",
        "Conv3x3 (256)\n",
        "Conv3x3 (256)\n",
        "Conv3x3 (256)\n",
        "MaxPool\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "MaxPool\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "Conv3x3 (512)\n",
        "MaxPool\n",
        "Fully Connected (4096)\n",
        "Fully Connected (4096)\n",
        "Fully Connected (1000)\n",
        "SoftMax\n",
        "The main purpose for which the VGG net was designed was to win the \n",
        "ILSVRC but it has been used in many other ways.\n",
        "\n",
        "Used just as a good classification architecture for many other \n",
        "datasets and as the authors made the models available to the \n",
        "public they can be used as is or with modification for other \n",
        "similar tasks also.\n",
        "Transfer learning : can be used for facial recognition tasks also.\n",
        "weights are easily available with other frameworks like keras so \n",
        "they can be tinkered with and used for as one wants.\n",
        "Content and style loss using VGG-19 network\n",
        "\n",
        "\n",
        "when i used vgg19 as a transfer learning i get result \n",
        "Private Score: 0.69048\n",
        "Public score: 0.68152\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EYMmv1fdoTbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trail6**"
      ],
      "metadata": {
        "id": "-eX-pABlosXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Plan_6 :**\n",
        "\n",
        "```\n",
        "Plan_6:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "\n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "\n",
        "we have input image,i will I will Cover Conv2d layer  \n",
        "and maxpolling is second and third layer then make flatten \n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax \n",
        "\n",
        "I will add Bidirectional layer \n",
        "```"
      ],
      "metadata": {
        "id": "-dgnjRvnoxln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3MSRjO9QCs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bfb481-6869-41d2-ee70-24b2d378b9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_48 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 62, 62, 32)   608         ['input_48[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_16 (MaxPooling2D  (None, 20, 20, 32)  0           ['conv2d_36[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_47 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 19, 19, 32)   4128        ['max_pooling2d_16[0][0]']       \n",
            "                                                                                                  \n",
            " embedding_21 (Embedding)       (None, 100, 100)     60000000    ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_17 (MaxPooling2D  (None, 9, 9, 32)    0           ['conv2d_37[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 40)          19360       ['embedding_21[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten_21 (Flatten)           (None, 2592)         0           ['max_pooling2d_17[0][0]']       \n",
            "                                                                                                  \n",
            " tf.concat_18 (TFOpLambda)      (None, 2632)         0           ['bidirectional_2[0][0]',        \n",
            "                                                                  'flatten_21[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            7899        ['tf.concat_18[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           63192       ['tf.concat_18[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,095,187\n",
            "Trainable params: 60,095,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Bidirectional # used to implement bidirectional RNNs (LSTM, GRU)\n",
        "\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2)) # input shape for batches of images (size 64x64x2)\n",
        "\n",
        "# text part\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text) # get our word embeddings\n",
        "bi_lstm = Bidirectional(LSTM(20))(embedded) # creating a bidirectional lstmlayer\n",
        "\n",
        "# image part\n",
        "cov = Conv2D(32, (3, 3))(in_image) # convolutional layer with 32 filters, no padding\n",
        "pl = MaxPool2D((3, 3))(cov) # max pooling with a 3x3 mask, reducing the size to 20x20x32\n",
        "cov2 = Conv2D(32, (2,2))(pl) # convolutional layer with 32 filters, and a 2x2 mask, no padding\n",
        "pl2 = MaxPool2D((2,2))(cov2) # max pooling with a 2x2 mask, reducing the size to 9x9x32\n",
        "flattened = Flatten()(pl2) # flatten the shape to (2592,)\n",
        "\n",
        "\n",
        "# fusion:\n",
        "fused = tf.concat([bi_lstm, flattened], axis=-1) # concatenate text features with images\n",
        "\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused) # predict price label 0, 1, or 2\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused) # predict rental category label [0-23]\n",
        "\n",
        "# model definition\n",
        "model6= keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text, # text inputs\n",
        "        'image': in_image # image inputs\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price, # label 1: price category (0, 1, or 2)\n",
        "        'type': p_type, # label 2: type category [0-23]\n",
        "    },\n",
        ")\n",
        "\n",
        "# compiling the bidirectional model and printing a summary of the architecture\n",
        "model6.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "model6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sfs41HIQCrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34cbb21-f0bf-49c3-b668-10c0bc3a1e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "359/359 [==============================] - 36s 88ms/step - loss: 10.4453 - price_loss: 9.8277 - type_loss: 11.0629 - price_sparse_categorical_accuracy: 0.5258 - type_sparse_categorical_accuracy: 0.5848 - val_loss: 3.7280 - val_price_loss: 2.5037 - val_type_loss: 4.9523 - val_price_sparse_categorical_accuracy: 0.6022 - val_type_sparse_categorical_accuracy: 0.7110\n",
            "Epoch 2/5\n",
            "359/359 [==============================] - 31s 87ms/step - loss: 2.4625 - price_loss: 1.9266 - type_loss: 2.9983 - price_sparse_categorical_accuracy: 0.5994 - type_sparse_categorical_accuracy: 0.6197 - val_loss: 3.2282 - val_price_loss: 1.9037 - val_type_loss: 4.5526 - val_price_sparse_categorical_accuracy: 0.6350 - val_type_sparse_categorical_accuracy: 0.3965\n",
            "Epoch 3/5\n",
            "359/359 [==============================] - 32s 90ms/step - loss: 4.7726 - price_loss: 2.7101 - type_loss: 6.8351 - price_sparse_categorical_accuracy: 0.6287 - type_sparse_categorical_accuracy: 0.6096 - val_loss: 9.4831 - val_price_loss: 7.0597 - val_type_loss: 11.9065 - val_price_sparse_categorical_accuracy: 0.6291 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 4/5\n",
            "359/359 [==============================] - 31s 85ms/step - loss: 2.6839 - price_loss: 1.6195 - type_loss: 3.7483 - price_sparse_categorical_accuracy: 0.6874 - type_sparse_categorical_accuracy: 0.6678 - val_loss: 1.9697 - val_price_loss: 1.5417 - val_type_loss: 2.3976 - val_price_sparse_categorical_accuracy: 0.6114 - val_type_sparse_categorical_accuracy: 0.6907\n",
            "Epoch 5/5\n",
            "359/359 [==============================] - 30s 84ms/step - loss: 1.0439 - price_loss: 0.8007 - type_loss: 1.2871 - price_sparse_categorical_accuracy: 0.7477 - type_sparse_categorical_accuracy: 0.7392 - val_loss: 1.6551 - val_price_loss: 1.3910 - val_type_loss: 1.9192 - val_price_sparse_categorical_accuracy: 0.6363 - val_type_sparse_categorical_accuracy: 0.6874\n"
          ]
        }
      ],
      "source": [
        "history = model6.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,\n",
        "    batch_size=17,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLho3WE_-iK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677cf710-9a4e-4ea3-9093-ccf397604f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.1532705e-01 1.7797069e-01 6.7022913e-03]\n",
            " [9.9998319e-01 1.3600852e-05 3.2428816e-06]\n",
            " [9.7554690e-01 1.6269948e-02 8.1832055e-03]\n",
            " ...\n",
            " [9.8380244e-01 1.5772868e-02 4.2462704e-04]\n",
            " [9.5845294e-01 2.1838030e-02 1.9709073e-02]\n",
            " [9.6778584e-01 1.5960269e-02 1.6253844e-02]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model6.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission6.csv', index=False)"
      ],
      "metadata": {
        "id": "psZJaB4NraDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation_6**\n",
        "\n",
        "```\n",
        "-Bidirectional recurrent neural networks (BRNN) connect two hidden \n",
        "layers of opposite directions to the same output. With this form \n",
        "of generative deep learning, the output layer can get \n",
        "information from past (backwards) \n",
        "\n",
        "I used Bidirectional layer \n",
        "the result is :\n",
        "Private Score: 0.63288\n",
        "Public score: 0.62826\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9Gy1mg72sM8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trail_7**"
      ],
      "metadata": {
        "id": "VYT6_WASsgvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cover Multi model and Multi objective"
      ],
      "metadata": {
        "id": "yReKeGnkGaPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Plan_7:\n",
        "in this trail I will use multi model multi objective to predict price and type \n",
        "but i will focus on price \n",
        "\n",
        "we have two input one is text and another is image \n",
        "in text part i will use empedding layer to train text input by max_len =150\n",
        "and i will take average for this layer then \n",
        "\n",
        "we have input image,i will I will  Cover three layers Conv2d layer  \n",
        "and three layers of  maxpolling then make flatten \n",
        "\n",
        "the output predict multilabel so i will use two dense layer \n",
        "with activation function softmax\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fQuWB4jqsp39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))  # input shape for batches of text sequences (100 words long)\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2)) # input shape for batches of images (size: 64x64x2)\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. \n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text) # get our word embeddings\n",
        "averaged = tf.reduce_mean(embedded, axis=1) # compute the mean of the embeddings to reduce the rank of the embedding\n",
        "\n",
        "# image part \n",
        "# simple conv2d.\n",
        "cov1 = Conv2D(16, 16, padding='same', activation='relu')(in_image)  # convolutional layer with  16x16 mask, no padding\n",
        "max1=MaxPooling2D()(cov1)#add maxpooling layer for first conv layer\n",
        "cov2 = Conv2D(16, 16, padding='same', activation='relu')(max1)  # convolutional layer with  16x16 mask, no padding\n",
        "max2=MaxPooling2D()(cov2)#add maxpooling layer for second conv layer\n",
        "conv3=Conv2D(64, 64, padding='same', activation='relu')(max2)# convolutional layer with  46x46 mask, no padding\n",
        "max3=MaxPooling2D()(conv3)#add maxpooling layer for third conv layer\n",
        "flattened = Flatten() (max3) # flatten the shape to (90,)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)# concatenate text features with images\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model7 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,# text inputs\n",
        "        'image': in_image# images inputs\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,  # label 1: type category [0-23]\n",
        "        'price': p_price # label 2: price category (0, 1, or 2)\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, \n",
        "# loss weights for each task.\n",
        "model7.compile(\n",
        "    optimizer= Adam(.001), # using Adam for optimization with learning rate (.001)\n",
        "\n",
        "    # measuring sparse categorical cross-entropy loss for both price and type labels\n",
        "    # sparse categorical cross-entropy is used since our labels are integers\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    # Not equally weight the loss w.r.t. both labels as we care more with price\n",
        "    loss_weights={\n",
        "        'type': 0,\n",
        "        'price': 1.0,       \n",
        "    },\n",
        "    # evaluation metric for this task is accuracy, so we measure sparse categorical accuracy for both targets\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "# display the structure of the model that we defined  \n",
        "model7.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzycqdzr8DA",
        "outputId": "5b6c5f49-9d46-4b3f-9ee6-703b231e3397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_54 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 64, 64, 16)   8208        ['input_54[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_21 (MaxPooling2D  (None, 32, 32, 16)  0           ['conv2d_41[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 32, 32, 16)   65552       ['max_pooling2d_21[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooling2D  (None, 16, 16, 16)  0           ['conv2d_42[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_53 (InputLayer)          [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 16, 16, 64)   4194368     ['max_pooling2d_22[0][0]']       \n",
            "                                                                                                  \n",
            " embedding_24 (Embedding)       (None, 100, 100)     60000000    ['input_53[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_23 (MaxPooling2D  (None, 8, 8, 64)    0           ['conv2d_43[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_14 (TFOpLa  (None, 100)         0           ['embedding_24[0][0]']           \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)           (None, 4096)         0           ['max_pooling2d_23[0][0]']       \n",
            "                                                                                                  \n",
            " tf.concat_20 (TFOpLambda)      (None, 4196)         0           ['tf.math.reduce_mean_14[0][0]', \n",
            "                                                                  'flatten_23[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            12591       ['tf.concat_20[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           100728      ['tf.concat_20[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 64,381,447\n",
            "Trainable params: 64,381,447\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model7.fit(\n",
        "    x={\n",
        "        'summary': df_train_text_id,\n",
        "        'image': df_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,\n",
        "    batch_size=17,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7KXEsMwOWf",
        "outputId": "3a6f34bb-b513-4cd2-df3b-b0df439fd2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "359/359 [==============================] - 51s 138ms/step - loss: 3.5436 - price_loss: 3.5436 - type_loss: 3.4885 - price_sparse_categorical_accuracy: 0.6122 - type_sparse_categorical_accuracy: 0.1326 - val_loss: 0.8078 - val_price_loss: 0.8078 - val_type_loss: 3.1038 - val_price_sparse_categorical_accuracy: 0.6278 - val_type_sparse_categorical_accuracy: 0.0321\n",
            "Epoch 2/5\n",
            "359/359 [==============================] - 48s 134ms/step - loss: 0.7793 - price_loss: 0.7793 - type_loss: 3.1014 - price_sparse_categorical_accuracy: 0.6402 - type_sparse_categorical_accuracy: 0.0364 - val_loss: 0.7495 - val_price_loss: 0.7495 - val_type_loss: 3.1115 - val_price_sparse_categorical_accuracy: 0.6717 - val_type_sparse_categorical_accuracy: 0.0013\n",
            "Epoch 3/5\n",
            "359/359 [==============================] - 48s 134ms/step - loss: 0.7187 - price_loss: 0.7187 - type_loss: 3.1078 - price_sparse_categorical_accuracy: 0.6874 - type_sparse_categorical_accuracy: 0.0928 - val_loss: 0.7313 - val_price_loss: 0.7313 - val_type_loss: 3.1070 - val_price_sparse_categorical_accuracy: 0.6664 - val_type_sparse_categorical_accuracy: 0.1972\n",
            "Epoch 4/5\n",
            "359/359 [==============================] - 48s 133ms/step - loss: 0.6798 - price_loss: 0.6798 - type_loss: 3.1121 - price_sparse_categorical_accuracy: 0.7094 - type_sparse_categorical_accuracy: 0.2432 - val_loss: 0.7063 - val_price_loss: 0.7063 - val_type_loss: 3.1041 - val_price_sparse_categorical_accuracy: 0.6861 - val_type_sparse_categorical_accuracy: 0.3263\n",
            "Epoch 5/5\n",
            "359/359 [==============================] - 48s 133ms/step - loss: 0.6508 - price_loss: 0.6508 - type_loss: 3.1163 - price_sparse_categorical_accuracy: 0.7266 - type_sparse_categorical_accuracy: 0.2577 - val_loss: 0.7019 - val_price_loss: 0.7019 - val_type_loss: 3.1154 - val_price_sparse_categorical_accuracy: 0.6841 - val_type_sparse_categorical_accuracy: 0.4659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model7.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzOJSIeHwW0g",
        "outputId": "701cacb0-d203-40c5-ea95-30a1a56d8c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.68444884 0.26925316 0.04629794]\n",
            " [0.8999072  0.08702339 0.01306946]\n",
            " [0.77799475 0.19038357 0.03162165]\n",
            " ...\n",
            " [0.7416923  0.22897474 0.02933288]\n",
            " [0.9364555  0.05246203 0.01108245]\n",
            " [0.6679747  0.28529713 0.0467282 ]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df_test.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('submission7.csv', index=False)"
      ],
      "metadata": {
        "id": "y01lFvzHx2wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation7**\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Max pooling is a type of operation that is typically added to CNNs \n",
        "following individual convolutional layers.\n",
        "\n",
        "When added to a model, max pooling reduces the dimensionality \n",
        "of images by reducing the number of pixels in the output from \n",
        "the previous convolutional layer.\n",
        "\n",
        "Max pooling operation for 2D spatial data. \n",
        "Downsamples the input along its spatial dimensions \n",
        "(height and width) by taking the maximum value over an \n",
        "input window (of size defined by pool_size ) for each channel of the input. \n",
        "The window is shifted by strides along each dimension\n",
        "\n",
        "why we used flatten?\n",
        "Rectangular or cubic shapes can't be direct inputs. And this is \n",
        "why we need flattening and fully-connected layers. Flattening is \n",
        "converting the data into a 1-dimensional array for inputting it to \n",
        "the next layer. We flatten the output of the convolutional layers to \n",
        "create a single long feature vector\n",
        "\n",
        "why we used embedding layer?\n",
        "\n",
        "Embedding layer enables us to convert each word into a fixed \n",
        "length vector of defined size. The resultant vector is a dense \n",
        "one with having real values instead of just 0's and 1's. The fixed \n",
        "length of word vectors helps us to represent words in a better way \n",
        "along with reduced dimensions.\n",
        "\n",
        "in this trail the result is : \n",
        "Private Score: 0.68967\n",
        "Public score: 0.67961\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "go02yIiGyNA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "aq6Gk3gqyZtA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "assignment4editting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e6fc2f02b244ab8b5c55bce0e2107aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_091d7fae07bc49008c27d88c533712a7",
              "IPY_MODEL_5aca1d0dd56c430d92ac777fe7c757e7",
              "IPY_MODEL_9dc0fb7248e54925adebf1aab7bfbec1"
            ],
            "layout": "IPY_MODEL_69b0315fb18d498a915d0489b8f190a0"
          }
        },
        "091d7fae07bc49008c27d88c533712a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6d4808d181b4c16afc67b1ea2bedd7f",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0c78d21ac5417594e4793ea9e0e0fd",
            "value": "100%"
          }
        },
        "5aca1d0dd56c430d92ac777fe7c757e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb6f1cdd1a34dbdaf5a085c77bddc6b",
            "max": 7627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3a795998cf34b8eb21d3681c8857bef",
            "value": 7627
          }
        },
        "9dc0fb7248e54925adebf1aab7bfbec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96cf89fd0939459db3cc4ae57fef2e6a",
            "placeholder": "​",
            "style": "IPY_MODEL_6d30c9a5ebac4a79b23cebd4796dee8d",
            "value": " 7627/7627 [01:34&lt;00:00, 106.49it/s]"
          }
        },
        "69b0315fb18d498a915d0489b8f190a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6d4808d181b4c16afc67b1ea2bedd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0c78d21ac5417594e4793ea9e0e0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceb6f1cdd1a34dbdaf5a085c77bddc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a795998cf34b8eb21d3681c8857bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96cf89fd0939459db3cc4ae57fef2e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d30c9a5ebac4a79b23cebd4796dee8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e128804ea04207b60adf44ccd4f068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9137cd0ff745423281834d7b249b39f5",
              "IPY_MODEL_8be5a3b58e5b449d9da904fdcac2bc5c",
              "IPY_MODEL_3a36a12cf9634c68ab13e0b02ad4ef2f"
            ],
            "layout": "IPY_MODEL_3fe331b6f6754d6b9808ce3b7fc89cc7"
          }
        },
        "9137cd0ff745423281834d7b249b39f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4d219b170ad4859bb43a35db17f62b2",
            "placeholder": "​",
            "style": "IPY_MODEL_7e24d849d7e24a7db919522b0beeb710",
            "value": "100%"
          }
        },
        "8be5a3b58e5b449d9da904fdcac2bc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13afae4fbbe447609930cd4a4f6a008d",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcf97e9dd33e4797b3ae125f338eded3",
            "value": 7360
          }
        },
        "3a36a12cf9634c68ab13e0b02ad4ef2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974c577e36894c859d58d6a5a8a1e204",
            "placeholder": "​",
            "style": "IPY_MODEL_feebde0b29b64158aed238203bfdc7db",
            "value": " 7360/7360 [01:18&lt;00:00, 96.64it/s]"
          }
        },
        "3fe331b6f6754d6b9808ce3b7fc89cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d219b170ad4859bb43a35db17f62b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e24d849d7e24a7db919522b0beeb710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13afae4fbbe447609930cd4a4f6a008d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf97e9dd33e4797b3ae125f338eded3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "974c577e36894c859d58d6a5a8a1e204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feebde0b29b64158aed238203bfdc7db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}